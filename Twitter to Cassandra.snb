{"metadata":{"name":"Getting Started with Spark and Cassandra","user_save_timestamp":"1970-01-01T01:00:00.000Z","auto_save_timestamp":"1970-01-01T01:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":"/tmp/repo","customRepos":null,"customDeps":["com.datastax.spark % spark-cassandra-connector_2.11 % 2.0.0-M3"],"customImports":null,"customArgs":null,"customSparkConf":{"spark.cassandra.connection.host":"172.17.0.2"}},"cells":[{"metadata":{"id":"B27ADAF4F2D14D22BF003C4BE71AE9CF"},"cell_type":"markdown","source":"#Getting Started with Spark and Cassandra"},{"metadata":{"id":"8F429C2988A746F9858E1CE9C4D029E2"},"cell_type":"markdown","source":"##This notebook shows you how to get Spark and Cassandra working together. \nTo try this notebook you will need an existing Cassandra instance, either a local server or a cluster.\nWe require the ip address of one seed node from that Cassandra single server or cluster.\n\n_This notebook is an adaptation of the [Quick Stark Guide](https://github.com/datastax/spark-cassandra-connector/blob/master/doc/0_quick_start.md) from the [Spark Cassandra Connector Repository](https://github.com/datastax/spark-cassandra-connector)_"},{"metadata":{"id":"90D4C542F8224C14AE7446A5EB6CF240"},"cell_type":"markdown","source":"##Create a keyspace and a table for our test"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"FAE34CA22AF44F809895EFD8E0E16FF7"},"cell_type":"code","source":"val createKeyspace = \"CREATE KEYSPACE IF NOT EXISTS test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };\" \nval createTable = \"CREATE TABLE IF NOT EXISTS test.kv(key text PRIMARY KEY, value int);\"","outputs":[{"name":"stdout","output_type":"stream","text":"createKeyspace: String = CREATE KEYSPACE IF NOT EXISTS test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };\ncreateTable: String = CREATE TABLE IF NOT EXISTS test.kv(key text PRIMARY KEY, value int);\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":1,"time":"Took: 1 second 491 milliseconds, at 2016-12-31 5:16"}]},{"metadata":{"id":"2B0308FEFC1A49338A2AECF718B499A7"},"cell_type":"markdown","source":"_We are going to use the Cassandra connector in \"manual\" mode to execute those statements_\nFor that, we first need the Spark Conf object that now contains all information we need to connect to Cassandra"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"EF7906B66C2B4BCA82F4DE38121CA926"},"cell_type":"code","source":"val sparkConf = sparkContext.getConf","outputs":[{"name":"stdout","output_type":"stream","text":"sparkConf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@225a9fc6\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":1,"time":"Took: 862 milliseconds, at 2016-12-31 5:20"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"908F81AB26D84C7D84861DA7A649E7D5"},"cell_type":"code","source":"import com.datastax.spark.connector.cql.CassandraConnector\nval cassandraConnector = CassandraConnector(sparkConf) ","outputs":[{"name":"stdout","output_type":"stream","text":"java.lang.NoClassDefFoundError: org/apache/spark/Logging\n  at java.lang.ClassLoader.defineClass1(Native Method)\n  at java.lang.ClassLoader.defineClass(ClassLoader.java:803)\n  at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n  at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)\n  at java.net.URLClassLoader.access$100(URLClassLoader.java:71)\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:412)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n  at java.lang.Class.getDeclaredMethods0(Native Method)\n  at java.lang.Class.privateGetDeclaredMethods(Class.java:2625)\n  at java.lang.Class.privateGetPublicMethods(Class.java:2743)\n  at java.lang.Class.getMethods(Class.java:1480)\n  at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.evalMethod(IMain.scala:848)\n  at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:781)\n  at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)\n  at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)\n  at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)\n  at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n  at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n  at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)\n  at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)\n  at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)\n  at notebook.kernel.Repl$$anonfun$7.apply(Repl.scala:208)\n  at notebook.kernel.Repl$$anonfun$7.apply(Repl.scala:208)\n  at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n  at scala.Console$.withOut(Console.scala:65)\n  at notebook.kernel.Repl.evaluate(Repl.scala:207)\n  at notebook.client.ReplCalculator$$anonfun$13$$anon$1$$anonfun$30.replEvaluate$1(ReplCalculator.scala:403)\n  at notebook.client.ReplCalculator$$anonfun$13$$anon$1$$anonfun$30.apply(ReplCalculator.scala:416)\n  at notebook.client.ReplCalculator$$anonfun$13$$anon$1$$anonfun$30.apply(ReplCalculator.scala:397)\n  at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n  at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n  at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)\n  at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)\n  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n  at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.Logging\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n  at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n  ... 42 more\n"}]},{"metadata":{"id":"770A6C7752344F7B87E8DEF038D5B755"},"cell_type":"markdown","source":"###Using the connector, execute the create statements"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"E186591B463043D98ED54F401D64CA1B"},"cell_type":"code","source":"cassandraConnector.withSessionDo { session =>\n  session.execute(createKeyspace)\n  session.execute(createTable)\n}","outputs":[]},{"metadata":{"id":"731174BCA4B64DEC9AA781DE9DDEA218"},"cell_type":"markdown","source":"###Insert some sample data"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"0FCDF788D0E541F58CACAE4B4690FC31"},"cell_type":"code","source":"CassandraConnector(sparkConf).withSessionDo { session =>\n  for (i <- 0 until 100) {\n    val insertStatement = s\"INSERT INTO test.kv(key, value) VALUES ('key${i}', $i);\"\n    session.execute(insertStatement)\n  }\n}","outputs":[]},{"metadata":{"id":"76061AEBCEAF4B87846BC784352D1790"},"cell_type":"markdown","source":"##Enable Cassandra-specific functions on the SparkContext and RDD:"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"ADAB055980FF409F8B408F87DD0C590C"},"cell_type":"code","source":"import com.datastax.spark.connector._","outputs":[]},{"metadata":{"id":"328EA71AFC4B49DE83102DA79DF60A92"},"cell_type":"markdown","source":"#Loading and analyzing data from Cassandra\nUse the `sc.cassandraTable` method to view this table as a Spark `RDD`:\n_This method will issue a full table load into Spark_"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"8D5FD9FCA1A742EB83DC27915B11ADD9"},"cell_type":"code","source":"val rdd = sc.cassandraTable(\"test\", \"kv\")","outputs":[]},{"metadata":{"id":"1AE794A737304F7AB6F4E4074A3B11F8"},"cell_type":"markdown","source":"###We can explore the data with Spark \n- count all elements"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"440FCC30559B4BDEA41E405B678D8CB1"},"cell_type":"code","source":"rdd.count","outputs":[]},{"metadata":{"id":"CD09307E72C24393B277A0CB25FF5FA3"},"cell_type":"markdown","source":"- get the first element"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"E995ABE17AD548558EE2FA159D5B5271"},"cell_type":"code","source":"rdd.first","outputs":[]},{"metadata":{"id":"F9A7F225AF5F4ADA819BC1A0C2BFAA7E"},"cell_type":"markdown","source":"- sum all the values"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"909A96111A814D3483D34C59B77F6C6E"},"cell_type":"code","source":"rdd.map(_.getInt(\"value\")).sum","outputs":[]},{"metadata":{"id":"8F94FCDEE6194C669376AC2A68085887"},"cell_type":"markdown","source":"- visualize the data"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"B7F5F1AFDACE48CE9136F99B92EDF0A2"},"cell_type":"code","source":"rdd.map(row =>  row.getInt(\"value\")).take(100)","outputs":[]},{"metadata":{"id":"A4D0A838557F4881AB168C70E3D4C841"},"cell_type":"markdown","source":"#Add Data from Spark to Cassandra\n###Create a new collection of data"},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":true,"id":"90D3F3C39D3948E7A387CBAFE82D1B7B"},"cell_type":"code","source":"val moreData = for (i <- 100 until 200) yield ((s\"addedKey$i\"),i) ","outputs":[]},{"metadata":{"id":"DD83C3AF43694E2482AB3E5D0EC6EA5F"},"cell_type":"markdown","source":"###Convert the collection into an RDD, \"parallelizing\" it"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"9823DA24F13A4A398C555CD652C883F7"},"cell_type":"code","source":"val moreDataRdd = sparkContext.parallelize(moreData) ","outputs":[]},{"metadata":{"id":"0DA9ACAAF33745139C4DE3099BA4F933"},"cell_type":"markdown","source":"### Store in Cassandra using the implicit methods on RDD"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"E15F53BD3E094AA2A20DAD31C16D5CB8"},"cell_type":"code","source":"moreDataRdd.saveToCassandra(\"test\",\"kv\")","outputs":[]},{"metadata":{"id":"553976BF10374CEA81903C17E1797FC7"},"cell_type":"markdown","source":"### Verify that the added data is there\n_Note that we don't need to re-declare the RDD. An action, such as `count` will trigger a re-computation of the RDD and therefore the new data will be taken into account_"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true,"id":"20E25EEEAEE1493E99C3F13F564ED04B"},"cell_type":"code","source":"rdd.count","outputs":[]}],"nbformat":4}